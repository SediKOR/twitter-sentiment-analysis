{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef27e257",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79fbf595",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer #Data transformation\n",
    "from sklearn.linear_model import LogisticRegression #Prediction Model\n",
    "from sklearn.metrics import accuracy_score #Comparison between real and predicted\n",
    "from sklearn.model_selection import train_test_split #Data testing\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras import models, layers, optimizers, losses, metrics\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt #Plotting properties\n",
    "#import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "from pprint import pprint\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4016b422",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./preprocessing_train.csv', index_col=0)\n",
    "val_data = pd.read_csv('./preprocessing_val.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ace1848",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y= [], []\n",
    "x = train_data['lower'].values.tolist()\n",
    "y = train_data['type'].values.tolist()\n",
    "\n",
    "x2,y2= [], []\n",
    "x2 = val_data['lower'].values.tolist()\n",
    "y2 = val_data['type'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c429dcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_percent = 0.2 # test data percent\n",
    "# make train and test data usin train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c774e387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39864"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train) # 31891\n",
    "len(x_test) # 7973\n",
    "len(x) # 39864\n",
    "len(y) # 39864"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f3812115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(train_data): # to make rnn model\n",
    "    train_data = tf.data.Dataset.from_tensor_slices(train_data)\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(1,), dtype=\"string\")) # input one string data (comment)\n",
    "    max_tokens = 15000 # dictionary size\n",
    "    max_len = 50 # comment to vectorize size\n",
    "    vectorize_layer = TextVectorization( # make textvectorization \n",
    "      max_tokens=max_tokens,\n",
    "      output_mode=\"int\",\n",
    "      output_sequence_length=max_len,\n",
    "    )\n",
    "    \n",
    "    vectorize_layer.adapt(train_data.batch(64))\n",
    "    model.add(vectorize_layer)\n",
    "    model.add(layers.Embedding(max_tokens + 1,output_dim= 200))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation=\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cc3f100b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dropout' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24648\\1803612113.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrnn_model\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m rnn_model.compile( # rnn model compile\n\u001b[0;32m      3\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m  \u001b[1;34m\"adam\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         metrics=['accuracy'])\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24648\\665845558.py\u001b[0m in \u001b[0;36mbuild_model\u001b[1;34m(train_data)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_tokens\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Dropout' is not defined"
     ]
    }
   ],
   "source": [
    "rnn_model =build_model(x_train)\n",
    "rnn_model.compile( # rnn model compile\n",
    "        optimizer=  \"adam\",\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "rnn_model.save(\"RNN_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d2073729",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 0.0068 - accuracy: 0.9974 - val_loss: 0.0448 - val_accuracy: 0.9834\n",
      "Epoch 2/5\n",
      "250/250 [==============================] - 18s 73ms/step - loss: 0.0069 - accuracy: 0.9970 - val_loss: 0.0404 - val_accuracy: 0.9871\n",
      "Epoch 3/5\n",
      "250/250 [==============================] - 17s 68ms/step - loss: 0.0066 - accuracy: 0.9974 - val_loss: 0.0456 - val_accuracy: 0.9834\n",
      "Epoch 4/5\n",
      "250/250 [==============================] - 18s 70ms/step - loss: 0.0065 - accuracy: 0.9970 - val_loss: 0.0471 - val_accuracy: 0.9852\n",
      "Epoch 5/5\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 0.0067 - accuracy: 0.9974 - val_loss: 0.0469 - val_accuracy: 0.9871\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_4 (TextV  (None, 50)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_4 (Embedding)     (None, 50, 200)           3000200   \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 10000)             0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 8)                 80008     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,080,217\n",
      "Trainable params: 3,080,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model training\n",
    "history = rnn_model.fit(x_train, y_train,\n",
    "                   epochs = 5,  batch_size = 128  , validation_data = (x2, y2) )\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e791f1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = tf.keras.models.load_model(\"./RNN_model\", custom_objects={\"TextVectorization\":TextVectorization})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
